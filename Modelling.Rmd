---
title: "Modelling"
author: "Lea Skaar-Henriksen"
date: "2024-11-30"
output: html_document
---

## Introduction

This includes the analysis of RNA-seq data using k-Nearest Neighbors (k-NN) and Random Forest models for classification. Variance Stabilizing Transformation (VST) was applied to normalize the data, followed by train-test splitting, model training, and evaluation.



## Load and Preprocess Data
The raw RNA-seq count data was loaded and preprocessed to remove unwanted features and low-quality samples. Genes containing "_PAR" were excluded, and samples with fewer than 20 million reads were filtered out to ensure sufficient coverage for downstream analysis.

```{r, message=FALSE, warning=FALSE}
# Load required libraries
library(dplyr)
library(tidyverse)
library(edgeR)
library(DESeq2)
library(caret)
library(class)
library(smotefamily)
library(randomForest)

# Read in data
data.counts <- readRDS("data_filtered.rds") %>%
  data.frame() %>%
  rownames_to_column(var = "Gene") %>%
  filter(!grepl("_PAR", Gene)) %>%
  mutate(Gene = sub("\\..*", "", Gene)) %>%
  column_to_rownames(var = 'Gene')

# Load sample information
sample_types_filtered <- readRDS("sample_types_filtered.rds")
names(sample_types_filtered) <- colnames(data.counts)
sample_types_filtered <- gsub(" ", "_", sample_types_filtered)

# Filter samples based on total counts â‰¥ 20M
sample_totals <- colSums(data.counts)
data.counts.new <- data.counts[, sample_totals >= 20E6]
sample_types_filtered <- sample_types_filtered[sample_totals >= 20E6]

# Check dimensions and missing values
dim(data.counts.new)
anyNA(data.counts.new)
```



## Normalize Data (VST)
Variance Stabilizing Transformation (VST) was applied to normalize the data. This method stabilizes variance and corrects for differences in sequencing depth, enabling fair comparisons across samples.
Perform Train-Test Split

```{r}
# Create a sample information table for all samples
sample_info <- data.frame(
  row.names = colnames(data.counts.new),
  condition = sample_types_filtered
)

# Create DESeqDataSet for the entire dataset
dds <- DESeqDataSetFromMatrix(
  countData = data.counts.new,
  colData = sample_info,
  design = ~condition
)

# Apply VST normalization to the entire dataset
vst_data <- vst(dds)
vst_matrix <- assay(vst_data)

```


## Train-test split

The dataset was divided into training (70%) and test (30%) sets following normalization. The larger portion will be used to train the model and the other to evaluate the model's performance on unseen data. A stratified splitting approach was used to maintain class proportions, ensuring that both sets are representative of the full dataset, and doesn't contain a skewed proportion of Tumor vs Normal samples.

```{r}

# Perform train-test split on VST-normalized data
set.seed(42)  # For reproducibility
train_idx <- createDataPartition(sample_types_filtered, p = 0.7, list = FALSE)

# Subset normalized data and labels
train_features <- t(vst_matrix[, train_idx])  # Transpose for modeling
test_features <- t(vst_matrix[, -train_idx])  # Transpose for modeling
train_labels <- as.factor(sample_types_filtered[train_idx])
test_labels <- as.factor(sample_types_filtered[-train_idx])

```



## Data Balancing

Our dataset exhibits a significant class imbalance, with far more Tumor samples (386) than Normal samples (41). Addressing this imbalance is crucial to ensure unbiased model performance and reliable predictions. Common approaches to tackle imbalance include undersampling the majority class, oversampling the minority class, generating syntethic samples or applying class weights.

For this analysis, we chose to apply weighting to address the imbalance. This method adjusts for class frequency by assigning weights inversely proportional to class size, ensuring that both classes are fairly represented during modeling. The benefits of weighting is that it retains all original data, preserving its integrity, and avoids the computational overhead of generating synthetic samples with other methods like SMOTE. Additionally, it is particularly effective for high-dimensional data, where oversampling methods may struggle with the curse of dimensionality. Also it is more efficient and requires less computational power.


```{r}
# Calculate class weights
class_counts <- table(train_labels)
class_weights <- max(class_counts) / class_counts
print(class_weights)
```
Weighting modifies the importance of each class by assigning weights inversely proportional to their frequencies. This way, predictions favor neither the majority nor the minority class. 


## Weighted k-Nearest Neighbors (kNN):



### Training the Model

```{r}
set.seed(42)
weighted_knn <- train(
  x = train_features,
  y = train_labels,
  method = "knn",
  tuneGrid = expand.grid(k = seq(3, 15, by = 2)),  # Odd k values
  trControl = trainControl(method = "cv", number = 10, classProbs = TRUE),
  weights = as.numeric(class_weights[train_labels])  # Apply class weights
)

# Display the best k value
cat("Best k value:", weighted_knn$bestTune$k, "\n")
```

The k-NN model was trained using weighted class frequencies to address imbalance. The optimal value of k, 9 was determined using 10-fold cross-validation.


### Making Predictions

```{r}
# Make predictions
predicted_labels <- predict(weighted_knn, newdata = test_features)
```


### Evaluating the model

```{r}
# Confusion matrix
confusion <- table(Real = test_labels, Predicted = predicted_labels)
print(confusion)

# Accuracy and error rate
accuracy <- sum(diag(confusion)) / sum(confusion)
cat("Accuracy:", accuracy, "\n")
error <- 1 - accuracy
cat("Error rate:", error, "\n")
```
The accuracy is very high. It may indicate overfitting or Data leakeage.



## Random Forest

Random Forest modelling is well-suited for high-dimensional data, performing effectively with a large number of features without the need for explicit dimensionality reduction. It is robust to overfitting due to its ensemble approach, which combines multiple decision trees to improve generalization and reduce variance. Additionally, Random Forest naturally handles class imbalance by allowing the adjustment of class weights (classwt), ensuring fair representation of minority classes during model training.

The same random split as used in the k-NN model was applied to maintain consistency.


### Training the Model

```{r}

# Train a random forest model
rf_model <- randomForest(
  x = train_features,
  y = train_labels,
  ntree = 500,              # Number of trees
  mtry = sqrt(ncol(train_features)),  # Features tried at each split
  classwt = as.numeric(class_weights),  # Apply class weights
  importance = TRUE         # Enable feature importance calculation
)

# Print model summary
print(rf_model)
```

A Random Forest model was trained using 500 trees and weighted class frequencies to address imbalance. Feature importance was calculated to identify key contributors to classification.

### Making Predictions
```{r}
# Make predictions
rf_predictions <- predict(rf_model, newdata = test_features)
```


### Evaluating the model

```{r}
# Confusion matrix
confusion <- table(Real = test_labels, Predicted = rf_predictions)
print(confusion)

# Accuracy and error rate
accuracy <- sum(diag(confusion)) / sum(confusion)
cat("Accuracy:", accuracy, "\n")
error <- 1 - accuracy
cat("Error rate:", error, "\n")
```
The confusion matrix, accuracy, and error rate were calculated to evaluate the performance of the Random Forest model.
The model has an accuracy of 100%. This could indicate several possibilities. It might be the model is showing excellent performance, with the data set being naturally well-separated. With only 29 samples in the Normal class, the model might perform perfectly due to limited diversity in the minority class. It could also be a result of overfitting, or data leakage. To confirm whether the high accuracy reflects true model performance or something is wrong, we will perform some further testing : 

### 1. Checking the Split
```{r}
# Check class distribution in training and test sets
cat("Training set class distribution:\n")
print(table(train_labels))

cat("Test set class distribution:\n")
print(table(test_labels))

```
The splits seem to be even.

        
### 2. Test for Data Leakage
```{r}
any(duplicated(rbind(train_features, test_features)))

```




### 3. Evaluate with Cross-Validation
```{r}
set.seed(42)
rf_cv <- train(
  x = train_features,
  y = train_labels,
  method = "rf",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = expand.grid(mtry = sqrt(ncol(train_features)))
)
print(rf_cv)

```
The high accuracy and kappa values indicate that the Random Forest model generalizes well across different subsets of the data, and there is no significant sign of overfitting. It seems the model is highly reliable for separating Primary Tumor and Solid Tissue Normal samples. The consistent results across folds highlight the model's reliability and its ability to handle the significant class imbalance effectively.





### Feature Importance

Feature importance is a metric used to identify the most influential features in a predictive model. In this analysis, it quantifies each feature's contribution to reducing classification uncertainty, measured by the Mean Decrease Gini. In other words, these genes makes the model able to separate the tumor and normal samples. Features with higher importance scores are key drivers of the model's accuracy and therefore may provide insights into the underlying biological processes affected in cancer cells. 


```{r}
# Extract feature importance
importance <- importance(rf_model)
feature_importance <- data.frame(
  Feature = rownames(importance),
  Importance = importance[, "MeanDecreaseGini"]
)

# Top 10 features
top_features <- feature_importance[order(-feature_importance$Importance), ][1:10, ]
print(top_features)
```

```{r}
# Plot feature importance
library(ggplot2)
ggplot(top_features, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(
    title = "Top 10 Most Important Features",
    x = "Feature (Gene)",
    y = "Importance (Mean Decrease Gini)"
  )

### Somwthing like this???
cancertable <- cancer.rf.model %>% 
  importance() %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Genes") %>%
  arrange(desc(MeanDecreaseAccuracy)) %>% 
  mutate(Genes = sub("\\..*", "", Genes))%>%
  left_join(gene_names, by = c("Genes" = "Ensembl.gene.ID")) %>% 
  na.omit()%>% 
  select("Genes", "MeanDecreaseAccuracy", "Symbol")

head(cancertable)

```

The plot visualizes the top 10 most important features in the Random Forest model based on their Mean Decrease Gini values. These features represent the genes that contribute most significantly to reducing impurity in the classification task.



## Biological Relevance


### 1. Map Ensembl IDs to Gene Symbols
```{r}
gene_names <- read.delim("genenames.org.txt", header = TRUE) %>%
  na.omit()

genetable <- rf_model %>% 
  importance() %>% 
  as.data.frame() %>% 
  rownames_to_column(var = "Genes") %>%
  mutate(Genes = sub("\\..*", "", Genes)) %>%
  left_join(gene_names, by = c("Genes" = "Ensembl.gene.ID")) %>%
  na.omit() %>%
  top_n(10, MeanDecreaseAccuracy) %>%
  arrange(desc(MeanDecreaseAccuracy)) %>%
  select("Genes", "MeanDecreaseAccuracy", "Symbol")

genetable


```


### 2. Functional Enrichment Analysis
The Gene Ontology (GO) enrichment analysis identified these significant biological processes associated with the top genes separating tumor and normal samples. These findings align with known cancer biology, emphasizing processes involved in cell growth, signaling, and regulation.

```{r}
knitr::include_graphics("GO Enrichment.PNG")
```

